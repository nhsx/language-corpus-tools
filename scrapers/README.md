Overview
===

This folder contains a web scraping framework based on [scrapy](https://scrapy.org/) (see the `scrapy` folder) and a collection of scrapers for various
sources (see the `scraper*` folders).

The framework is responsible for making sure there are no duplicates and for storing the documents in the appropriate target systems (currently MedCATTrainer
and Doccano for free text items and datadict database for DataDictionary items).

The scrapers produce a sequence of Requests and callbacks for handling responses. The callbacks are expected to generate Items that are then handled by
the framework. Note that the scrapers should not need to access databases or call internal APIs, this is the framework's responsibility.

In order to detect and avoid duplicates the framework maintains a database of previously downloaded documents. The database contains just one table of
the following structure:

```sql
CREATE TABLE IF NOT EXISTS documents (
    source_id VARCHAR(32),
    url VARCHAR(255),
    modified_at TIMESTAMP WITH TIME ZONE,
    CONSTRAINT unique_doc UNIQUE(url)
)
```

Each Request is checked against this table (unless `force_refresh` meta attribute is set to `True`). If a document with the same URL is found an
`If-Modified-Since` header is added (unless `dont_refresh` meta attribute is set to `True` in which case the Request is ignored). If the server responds
with a 304 (Not Modified) response, the response is dropped.


Packaging
===

The framework is packaged as a Docker image that serves as a base for scrapers images. The scaper images may contain one or more scrapers. The breakdown
should be guided by the scheduling needs: if it's ok for some scrapers to be run at the same time they can go into the same image.

When a scraper image is run, it automatically launches all scrapers found in the `spiders` folder simultaneously. The container exits when the last
spider finishes.

Developing Scrapers
===

In order to add a scraper first decide whether it goes into a separate Docker image or it could be added to an existing one (see Packaging). If it needs
to go into a separate image, create a uniquely named `spider_*` or `spiders_*` folder (depending on whether there are single or multiple spiders). Make
sure to add the Dockerfile and Makefile (use an existing one as a template).

Then add a spider(s) implementation as a python module. The spider class must have the `source_id` attribute. It is used to identify the source from
which a document was ingested.

Each `Item` generated by a scraper must have `source_url` field set. Note that this field should uniquely identify a _document_, not a page. Hence, if there
are multiple documents on the same page the url should be a combination of the page url and some unique identified for the document. If the source does
not provide such an identifier an MD5 or SHA1 hash of the text could be used instead. Note that `modified_at` attribute should not be set and that the
page caching mechanism won't be effective in this case (i.e. the page will always be refreshed).

Running Scrapers
===

Depending on the deployment type there are two ways of running the scrapers: either as a Docker container or locally. The latter is currently only works
with local-docker deployment.

In order to launch a scraper in AWS or in a local Docker use `run_scraper.sh` in the corresponding folder (appstack/aws-ecs or appstack/local-docker).

In order to run a scraper in a local Docker use `appstack/local-docker/run_scraper_local.sh`.
